{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Question\n",
    "\n",
    "Given a list of timestamps in sequential order, return a list of lists grouped by week (7 days) using the first timestamp as the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [\n",
    "    '2019-01-01', \n",
    "    '2019-01-02',\n",
    "    '2019-01-08', \n",
    "    '2019-02-01', \n",
    "    '2019-02-02',\n",
    "    '2019-02-05',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_week(series):\n",
    "    \n",
    "    series = sorted(series) #works only if days and months are zero paded\n",
    "    dt_series = pd.to_datetime(series)\n",
    "    week = (dt_series - dt_series[0]).days//7\n",
    "    \n",
    "    out = []\n",
    "    for i in week.unique():\n",
    "        out.append([series[j] for j in range(0, len(series)) if week[j] == i])\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2019-01-01', '2019-01-02'],\n",
       " ['2019-01-08'],\n",
       " ['2019-02-01', '2019-02-02'],\n",
       " ['2019-02-05']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_week(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Question\n",
    "\n",
    "Let's say you work for a social media company that has just done a launch in a new city. Looking at weekly metrics, you see a slow decrease in the average number of comments per user from January to March in this city.\n",
    "\n",
    "The company has been consistently growing new users in the city from January to March.\n",
    "\n",
    "What are some reasons on why the average number of comments per user would be decreasing and what metrics would you look into?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "It is normal that users number is increasing but the average comments per user are decreasing. Because the average comment per user is calculated by: total number of comments/total number of users. Not every user will comment while the number of user is increasing.So the decreasing number of comments is meaningless.\n",
    "\n",
    "As a newly launched company, some of key metrics should be looked into, for example, monthly active users, retained users.\n",
    "\n",
    "While the number of new user is increasing, the company has to pay attention to number of retained users. Sometimes the number of retained users are very small even though a lot of new users, that means users dont use our service after first time;Sometimes new users growing very slow, but we have a pretty stable number of retained users, that means we are doing good but the market is probably saturated, we have to figure out how to bring more new customers.\n",
    "\n",
    "Those are some examples of key metrics new companies should pay attention to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Question\n",
    "\n",
    "Imagine a deck of 500 cards numbered from 1 to 500. If all the cards are shuffled randomly and you are asked to pick three cards, one at a time, what's the probability of each subsequent card being larger than the previous drawn card?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "We can use permutations and combinations to solve this problem.\n",
    "\n",
    "- Number of ways of picking cards in increasing order will be 500 choose 3 (unordered) i.e. any 3 cards chosen can be arranged in the increasing order\n",
    "\n",
    "- Total number of ways to pick 3 cards will be 500 permute 3 (ordered)\n",
    "\n",
    "P = 500C3 / 500P3\n",
    "P = 1/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Question\n",
    "\n",
    "Suppose we have a binary classification model that classifies whether or not an applicant should be qualified to get a loan. Because we are a financial company we have to provide each rejected applicant with a reason why.\n",
    "\n",
    "Given we don't have access to the feature weights, how would we give each rejected applicant a reason why they got rejected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Let's pretend that we have three people: Alice, Bob, and Candace that have all applied for a loan. Simplifying the financial lending loan model, let's assume the only features are total number of credit cards, dollar amount of current debt, and credit age.\n",
    "\n",
    "Let's say Alice, Bob, and Candace all have the same number of credit cards and credit age but not the same dollar amount of curent debt.\n",
    "\n",
    "- Alice: 10 credit cards, 5 years of credit age, $20K of debt\n",
    "\n",
    "- Bob: 10 credit cards, 5 years of credit age, $15K of debt\n",
    "\n",
    "- Candace: 10 credit cards, 5 years of credit age, $10K of debt\n",
    "\n",
    "Alice and Bob get rejected for a loan but Candace gets approved. We would assume that given this scenario, we can logically point to the fact that Candace's 10K of debt has swung the model to approve her for a loan.\n",
    "\n",
    "How did we reason this out? If the sample size analyzed was instead thousands of people who had dthe same number of credit cards and credit age with varying levels of debt, we could figure out the model's average loan acceptance rate for each numerical amount of current debt. Then we could plot these on a graph to model out the y-value, average loan acceptance, versus the x-value, dollar amount of current debt.\n",
    "\n",
    "These graphs are called partial depedence plots!\n",
    "\n",
    "The partial dependence plot is calculated only after the model has been fit. The model is fit on the real data. In that real data, loans are given dependent on different feaures. But after the model is fit, we could start by taking all the characteristics of a loan and plotting them against the dependent variable whilst keeping all of the other features the same except for the one feature variable we want to test.\n",
    "\n",
    "We then use the model to predict the loan qualification but we change the debt dollar amount before making a prediction. We first predict the loan qualification for an example person by setting it to 20K. We then predict it again at 19K. Then predict again for 18K. And so on. We trace out how predicted probability of loan qualification (on the vertical axis) as we move from small values of debt dollar amount to large values (on the horizontal axis). This way we are able to see how a model's features affect the score without digging into the classifier feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Question\n",
    "\n",
    "Given a table of students and their SAT test scores, write a query to return the two students with the closest test scores with the score difference. Assume a random pick if there are multiple students with the same score difference.\n",
    "\n",
    "`scores` table\n",
    "\n",
    "|column | type |\n",
    "--------|-------\n",
    "|id     | integer|\n",
    "|student | varchar|\n",
    "|score | integer|\n",
    "\n",
    "[Link](https://mail.google.com/mail/u/0/#search/team%40interviewquery.com/FMfcgxwGBmwFdpbjxTxpfGtdHTTtTvKq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT a.Student AS one_student, b.Student AS other_setudent,\n",
    "       ABS(a.Score - b.Score) AS score_diff\n",
    "FROM scores AS a\n",
    "JOIN scores AS b\n",
    "WHERE a.Id < b.Id\n",
    "ORDER BY score_diff\n",
    "LIMIT 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Question\n",
    "\n",
    "Given a stream of numbers, select a random number from the stream, with O(1) space in selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Question\n",
    "\n",
    "There are four people on the ground floor of a building that has five levels not including the ground floor. They all get into the same elevator.\n",
    "\n",
    "If each person is equally likely to get on any floor and they leave independently of each other, what is the probability that no two passengers will get off at the same floor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "The number of ways to assing 5 floors to 4 passengers is 5⋅5⋅5⋅5 beacuse for each passenger you can choose one of the 5 floors.\n",
    "\n",
    "The number of ways to assign 5 floors to 4 passengers without repetition of floors is 5⋅4⋅3⋅2 because for the first passenger you have 5 option, for the second you will have 4 and so on. Note that this number count all possible orders betwen passengers too.\n",
    "\n",
    "Then, you will guess it, the result is\n",
    "\n",
    "24/125."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Question\n",
    "\n",
    "[Link](https://mail.google.com/mail/u/0/#search/team%40interviewquery.com/FMfcgxwGBwSgjkxrvkQFknpWXpmWDkdM)\n",
    "\n",
    "`attribution` table\n",
    "\n",
    "#### column type\n",
    "- id\tint\n",
    "- created_at\tdatetime\n",
    "- session_id\tint\n",
    "- channel\tvarchar\n",
    "- conversion\tboolean\n",
    "\n",
    "`user_sessions` table\n",
    "\n",
    "#### column\tuser_id\n",
    "- session_id\tint\n",
    "- user_id\tint\n",
    "\n",
    "The schema above is for a retail online shopping company.\n",
    "\n",
    "The attribution table logs each user visit where a user comes onto their site to go shopping. If conversion = 1, then on that session visit the user converted and bought an item. The channel column represents which advertising platform the user got to the shopping site on that session. The `user_sessions` table maps each session visit back to the user.\n",
    "\n",
    "First touch attribution is defined as the channel to which the converted user was associated with when they first discovered the website. Calculate the first touch attribution for each user_id that converted.\n",
    "\n",
    "Example output:\n",
    "\n",
    "#### user_id\tchannel\n",
    "- 123\tfacebook\n",
    "- 145\tgoogle\n",
    "- 153\tfacebook\n",
    "- 172\torganic\n",
    "- 173\temail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT T.user_id, a2.channel FROM  \n",
    "\n",
    "(SELECT u.user_id, MIN(u.session_id) FROM attribution AS a\n",
    "JOIN user_sessions AS u ON a.session_id = u.session_id\n",
    "GROUP BY u.user_id) AS T\n",
    "\n",
    "JOIN attribution AS a2 ON T.session_id = a2.session_id\n",
    "WHERE a2.conversion = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Question\n",
    "\n",
    "Let's say we have 1 million Lyft rider journey trips in the city of Seattle. We want to build a model to predict ETA after a rider makes a Lyft request.\n",
    "\n",
    "How would we know if we have enough data to create an accurate enough model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Question is definitely unanswerable without knowing the business metrics. What is “accurate enough” for the business?\n",
    "\n",
    "I would definitely mention train/test/validate datasets (maybe 70%/20%/10%), but I think there’s a way to guess whether a model will actually work before building it.\n",
    "\n",
    "Could try to fit distributions to the ETAs as functions of individual variables (holding others constant) and seeing what families you get. If distributions are normal-ish (i.e. from distributions that quickly converge), you’re likely safe. If distributions look more like power laws, you may need more data.\n",
    "\n",
    "The distribution lens depends on the price of extreme values. How much does it really matter if we forecast 5 minutes and it ends up being 20? Also a business quesiton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Question\n",
    "\n",
    "There are two tables. One table is called `swipes` that holds a row for every Tinder swipe and contains a boolean column that determines if the swipe was a right or left swipe called `is_right_swipe`. The second is a table named `variants` that determines which user has which variant of an AB test.\n",
    "\n",
    "Write a SQL query to output the average number of right swipes for two different variants of a feed ranking algorithm by comparing users that have swiped the first 10, 50, and 100 swipes on their feed.\n",
    "\n",
    "Tip: Users have to have swiped at least 10 times to be included in the subset of users to analyze the mean number of right swipes.\n",
    "\n",
    "SQL: [link](https://mail.google.com/mail/u/0/#create-filter/has=team%40interviewquery.com&sizeoperator=s_sl&sizeunit=s_smb&query=team%40interviewquery.com/FMfcgxwGBwbZshBKVbgcjHTjBKFHvLzr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Question\n",
    "\n",
    "Pretend you have to analyze the results of an AB test. One variant of the AB test has a sample size of 50K users and the other has a sample size of 200K users.\n",
    "\n",
    "Given the unbalanced size between the two groups, can you determine if the test will result in bias towards the smaller group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Question\n",
    "\n",
    "Let's say you're a product data scientist at Facebook. Facebook is rolling out a new feature called \"Mentions\" which is an app specifically for celebrities on Facebook to connect with their fans.\n",
    "\n",
    "How would you measure the health of the Mentions app? And if a celebrity starts using Mentions and begins interacting with their fans more, what part of the increase can be attributed to a celebrity using Mentions versus what part is just a celebrity wanting to get more involved in fan engagement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Question\n",
    "\n",
    "Given a transaction table of product purchases, write a query to get the number of customers that were upsold by purchasing additional products.\n",
    "\n",
    "Note that if the customer purchased two things on the same day that does not count as an upsell as they were purchased within a similar timeframe. Each row in the transactions table also represents an individual user product purchase.\n",
    "\n",
    "transactions table\n",
    "\n",
    "#### column\ttype\n",
    "- user_id\tint\n",
    "- created_at\tdatetime\n",
    "- product_id\tint\n",
    "- quantity\tint\n",
    "- price\tfloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Question \n",
    "\n",
    "You have an array of integers of length n spanning 0 to n with one missing. Write a function that returns the missing number in the array\n",
    "\n",
    "Example:\n",
    "\n",
    "nums = [0,1,2,4,5] \n",
    "\n",
    "missingNumber(nums) -> 3\n",
    "\n",
    "Complexity of O(N) required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nums = [0, 1, 2, 4, 5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingNumber(arr):\n",
    "\n",
    "    total = np.sum(arr)\n",
    "    n = len(arr)\n",
    "    actual = n*(n+1)/2\n",
    "    \n",
    "    return actual - total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missingNumber(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Question\n",
    "\n",
    "You are about to get on a plane to Seattle. You want to know if you should bring an umbrella. You call 3 random friends of yours who live there and ask each independently if it's raining. Each of your friends has a 2/3 chance of telling you the truth and a 1/3 chance of messing with you by lying. All 3 friends tell you that \"Yes\" it is raining.\n",
    "\n",
    "What is the probability that it's actually raining in Seattle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P(rain/yyy) = P(rain.yyy) / P(yyy)\n",
    "\n",
    "P(rain/yyy) = P(yyy/rain).P(rain) / [P(yyy/rain).P(rain) + P(yyy/no_rain).P(no_rain)]\n",
    "\n",
    "P(rain/yyy) = [(2/3)^3 . Pr] / [(2/3)^3 . Pr + (1/3)^3 . Pn]\n",
    "\n",
    "P(rain/yyy) = 8 . Pr / [8 . Pr + Pn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Question\n",
    "\n",
    "SQL: [link](https://mail.google.com/mail/u/0/#create-filter/has=team%40interviewquery.com&sizeoperator=s_sl&sizeunit=s_smb&query=team%40interviewquery.com/FMfcgxwGCQTctQDpvmHJBpQPxLmDGjvX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Question\n",
    "\n",
    "Suppose there exists a new airline named Jetco flies domestically across North America. Jetco recently had an study commissioned that tested the boarding time of every airline and it came out that Jetco had the fastest average boarding times of any airline.\n",
    "\n",
    "What factors could have biased this result and what would you look into?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Jetco had the fastest average borading time. First of all I would look into the sample size and how many flights of the data we have. If we are comparing the average of 5 flights of Jetco vs average of 50 flights of another flights, then it would be biased.\n",
    "\n",
    "\n",
    "Also, what is the averge number of passangers that travels through jetco compared to other flights. If number of passengers are less, then boarding time would be minimum anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Question\n",
    "\n",
    "Given an array of words and a maxWidth parameter, format the text such that each line has exactly maxWidth characters. Pad extra spaces ' ' when necessary so that each line has exactly maxWidth characters.\n",
    "\n",
    "Extra spaces between words should be distributed as evenly as possible. If the number of spaces on a line do not divide evenly between words, the empty slots on the left will be assigned more spaces than the slots on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example:\n",
    "\n",
    "words = [\"This\", \"is\", \"an\", \"example\", \"of\", \"text\", \"justification.\"]\n",
    "maxWidth = 16\n",
    "\n",
    "Output = [\n",
    "   \"This    is    an\",\n",
    "   \"example  of text\",\n",
    "   \"justification.  \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def merge(arr, width):\n",
    "    \n",
    "    l = len(arr)\n",
    "    l1 = len(' '.join(arr))\n",
    "    \n",
    "    if l1 == width:\n",
    "        return ' '.join(arr)\n",
    "    else:\n",
    "        diff = width - len(''.join(arr))\n",
    "        \n",
    "        if l == 1:\n",
    "            return arr[0] + ' '*diff\n",
    "        elif diff%(l-1) == 0:\n",
    "            return (' '*(diff//(l-1))).join(arr)\n",
    "        else:\n",
    "            left = int(np.ceil(diff/(l-1)))\n",
    "            right = int(np.floor(diff/(l-1)))\n",
    "            n_space = l-1\n",
    "            \n",
    "            spaces = [left] + [right for i in range(0, n_space-1)] + [0]\n",
    "            \n",
    "            return ''.join([x + ' '*y for x, y in zip(arr,spaces)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_format_width(arr, width):\n",
    "    \n",
    "    l = 0\n",
    "    words_arr = []\n",
    "    output = []\n",
    "\n",
    "    for i in words:\n",
    "\n",
    "        l2 = l + len(i)\n",
    "        if l2 > maxWidth:\n",
    "            output.append(merge(words_arr, maxWidth))\n",
    "            words_arr = [i]\n",
    "            l = len(i)\n",
    "        else:\n",
    "            words_arr.append(i)\n",
    "            l = l2 + 1 \n",
    "\n",
    "    output.append(merge(words_arr, maxWidth))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This    is    an', 'example  of text', 'justification.  ']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_format_width(words, maxWidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 16, 16]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in txt_format_width(words, maxWidth)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Question\n",
    "\n",
    "SQL: [link](https://mail.google.com/mail/u/0/#create-filter/has=team%40interviewquery.com&sizeoperator=s_sl&sizeunit=s_smb&query=team%40interviewquery.com/FMfcgxwGCQfpJTlMPSrfphrrpMkXJnGb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT T.day, AVG(val) AS rate FROM\n",
    "\n",
    "(SELECT p.user_id, DAYOFWEEK(p.created_at) AS day,\n",
    "CASE WHEN P.event_name = 'post' THEN 1 ELSE 0 END AS val\n",
    "FROM post_events AS p\n",
    "WHERE p.event_name = 'post' OR p.event_name = 'cancel') AS T\n",
    "\n",
    "GROUP BY T.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Question\n",
    "\n",
    "In data science, there exists the concept of stemming, which is the heuristic of chopping off the end of a word to clean and bucket it into an easier feature set. \n",
    "\n",
    "Given a dictionary consisting of many roots and a sentence, stem all the words in the sentence with the root forming it. If a word has many roots can form it, replace it with the root with the shortest length.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input: \n",
    "- roots = [\"cat\", \"bat\", \"rat\"]\n",
    "- sentence = \"the cattle was rattled by the battery\"\n",
    "\n",
    "Output: \"the cat was rat by the bat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = [\"cat\", \"bat\", \"rat\"]\n",
    "sentence = \"the cattle was rattled by the battery\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word, roots):    \n",
    "    for r in roots:\n",
    "        if r in word:\n",
    "            st = r\n",
    "            break\n",
    "        st = word\n",
    "    return st   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat was rat by the bat'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(' ').join([stem(i, roots) for i in sentence.split(' ')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
